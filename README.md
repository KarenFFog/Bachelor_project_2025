**Project description** 

This project investigates the potential of using geographic descriptions generated by Large Language Models (LLMs) to
improve the learning of satellite image representations. Many aspects of geographic locations—such as cultural,
historical, or socio-economic information—are not directly visible in satellite images. LLMs offer a way to capture and
incorporate such information into the learning process.

The project focuses on generating descriptive text for geolocations in the MMEarth dataset using LLMs. These
descriptions will serve as a target during the pretraining phase of an image encoder for Sentinel-2 optical images. The
aim is to explore whether incorporating this textual data can enhance the quality of the learned image representations.
The generated descriptions are expected to be inherently noisy, presenting a challenge in learning representations that
generalize well to downstream tasks. This noise level and its impact on the learning process will be analyzed to identify
the performance gaps when pretraining with descriptions generated by different LLMs and employing varied prompting
strategies.

This project is inspired by several recent advancements
- The MMEarth dataset [1], which demonstrates the power of combining multimodal data (e.g., optical images, elevation
data, and landcover) from 1.2 million geolocations for representation learning in Earth observation.
They use a Multi-Pretext Masked Autoencoder (MP-MAE) to pretrain models on multi-modal data, outperforming
ImageNet-based pretraining by learning richer features for geospatial tasks like landcover classification, crop mapping,
and climate zone prediction.
- GeoLLM [2], which revealed that LLMs, trained on vast language corpora, possess substantial geospatial knowledge
that can be extracted to enhance tasks like population density estimation and economic livelihood prediction
- The integration of text and spatial observations for tasks like species range estimation [3], which highlighted how
language-based descriptions can provide semantic context unavailable in image data, enabling zero-shot and few-shot
learning in ecology.


**Learning goals:**
- Get hands-on experience with applying LLMs for large synthetic dataset creation. Explore different prompting
strategies to improve their outputs for geospatial tasks.
- Learn about representation learning for images with textual pretext tasks by extending existing learning pipelines with
new pretraining tasks. Investigate different strategies for using textual data as pretext tasks such as embedded text
with pretrained text encoders or image-captioning approaches.
- Analyze the performance gaps when pretraining with descriptions generated by different LLMs and using varied
prompting strategies to understand their impact on downstream tasks.


**References**

- [1] Nedungadi, V., Kariryaa, A., Oehmcke, S., Belongie, S., Igel, C., \& Lang, N. (2024). **MMEarth: Exploring Multi-Modal Pretext Tasks for Geospatial Representation Learning**.
- [2] Manvi, R., Khanna, S., Mai, G., Burke, M., Lobell, D., \& Ermon, S. (2024). **GeoLLM: Extracting Geospatial Knowledge from Large Language Models**.
- [3] Hamilton, M., Lange, C., Cole, E., Shepard, A., Heinrich, S., Mac Aodha, O., Van Horn, G., \& Maji, S. (2024).**Combining Observational Data and Language for Species Range Estimation**.
 
